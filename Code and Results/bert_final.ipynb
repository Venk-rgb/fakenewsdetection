{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional Encoder Representations from Transformers (BERT)\n",
    "\n",
    "The following code downloads and fits the pretrained BERT model for the fake news detection task using the LIAR dataset. The steps involved are:\n",
    "1. Load and preprocess the data.\n",
    "2. Download the pretrained model using tensorflow hub and implement a function to perform BERT tokenization.\n",
    "3. Initialize a dataloader that will effectively load the data into the model and use an ADAM optimizer to optimize the model parameters.\n",
    "4. A `train_epoch` and `evaluate_epoch` function that will train the BERT model on batches of train data and evaluates the performance for each epoch.\n",
    "5. Final code cells to print the confusion matrix and classification report to evaluate our model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading and preprocessing data\n",
    "This code cell is designed to load, preprocess, and analyze a dataset, presumably for a machine learning task related to text classification. It begins by defining two functions: `load_and_preprocess_data` and `preprocess_data`. The first function loads data from a file, and assigns specified column names. The second function preprocesses this data by creating a binary label based on certain conditions. For the sake of our project, we assigned \"true\" and \"mostly-true\" classes as 1, and the rest of the classes as 0. This will help us in our binary classification task. The function also removes certain columns, combines multiple metadata columns into a single column, and then appends this metadata to the text statements. After defining these functions, the code loads training, testing, and validation datasets from specified file paths and applies the preprocessing steps. Finally, it calculates and prints the count of positive and negative class labels in each dataset, providing an overview of the class distribution in the training, testing, and validation sets. This is useful for understanding the balance of classes in the data, which is crucial for training effective machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rhWZMJlRO_k9",
    "outputId": "f104b7fe-677a-489a-aaf8-3804921707c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of negative class values in train data = 4207\n",
      "Count of positive class values in train data = 2517\n",
      "Count of negative class values in test data = 531\n",
      "Count of positive class values in test data = 322\n",
      "Count of negative class values in validation data = 564\n",
      "Count of positive class values in validation data = 297\n"
     ]
    }
   ],
   "source": [
    "# This code cell performs the following operations:\n",
    "import pandas as pd\n",
    "\n",
    "def load_and_preprocess_data(filepath, column_names):\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    data.columns = column_names\n",
    "    return data\n",
    "\n",
    "def preprocess_data(data):\n",
    "    data['label_to_predict'] = data['label'].apply(lambda x: 1 if x in [\"true\", \"mostly-true\"] else 0)\n",
    "    data = data.drop(['ID', 'label', 'barely_true_count', 'false_count', 'half_true_count', 'mostly_true_count', 'pants_on_fire_count'], axis=1)\n",
    "\n",
    "    # Combining metadata into a single column\n",
    "    metadata_columns = ['subject', 'speaker', 'speaker_job_title', 'state_info', 'party_affiliation', 'statement_context']\n",
    "    data['metadata'] = data[metadata_columns].fillna('None').agg(' '.join, axis=1)\n",
    "\n",
    "    # Final sentence column\n",
    "    data['sentence'] = data['metadata'] + \" \" + data['statement']\n",
    "    return data.dropna()\n",
    "\n",
    "def print_class_counts(datasets):\n",
    "    for name, dataset in datasets.items():\n",
    "        neg_count = len(dataset[dataset['label_to_predict'] == 0])\n",
    "        pos_count = len(dataset[dataset['label_to_predict'] == 1])\n",
    "        print(f'Count of negative class values in {name} data = {neg_count}')\n",
    "        print(f'Count of positive class values in {name} data = {pos_count}')\n",
    "\n",
    "# Column names\n",
    "column_names = ['ID', 'label', 'statement', 'subject', 'speaker', 'speaker_job_title', 'state_info', 'party_affiliation', 'barely_true_count', 'false_count', 'half_true_count', 'mostly_true_count', 'pants_on_fire_count', 'statement_context']\n",
    "\n",
    "# Load and preprocess data\n",
    "train_data = load_and_preprocess_data('/content/drive/MyDrive/Foundations of Data Science/Project/Data/train.tsv', column_names)\n",
    "test_data = load_and_preprocess_data('/content/drive/MyDrive/Foundations of Data Science/Project/Data/test.tsv', column_names)\n",
    "validation_data = load_and_preprocess_data('/content/drive/MyDrive/Foundations of Data Science/Project/Data/valid.tsv', column_names)\n",
    "\n",
    "# Apply preprocessing\n",
    "cleaned_train_data = preprocess_data(train_data)\n",
    "cleaned_test_data = preprocess_data(test_data)\n",
    "cleaned_val_data = preprocess_data(validation_data)\n",
    "\n",
    "# Print class counts\n",
    "print_class_counts({'train': cleaned_train_data, 'test': cleaned_test_data, 'validation': cleaned_val_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting up text data for BERT training\n",
    "\n",
    "This code cell sets up a BERT (Bidirectional Encoder Representations from Transformers) model for sequence classification, specifically for a binary classification task. It initializes the model and tokenizer from the pre-trained 'bert-base-uncased' model with `num_labels=2` as we are modelling a binary classification task. The model is then moved to GPU for efficient computation, if a GPU is available. A custom function, bert_tokenization, is defined to tokenize datasets using the BERT tokenizer. This function takes a dataset and an optional argument to decide whether to use additional metadata or just the statement text. It processes each sentence in the dataset to generate BERT's input IDs and attention masks, which are required for the model to understand which parts of the input are meaningful and should be focused on. These are then converted into PyTorch tensors, suitable for input to the BERT model. Finally, the function tokenizes the training, testing, and validation datasets prepared earlier, resulting in tensors of input IDs, attention masks, and labels for each set, ready for use in training and evaluating the BERT model. This setup is a typical workflow for preparing text data for training with a pre-trained transformer model in natural language processing tasks.\n",
    "\n",
    "**BERT's Input IDs:**\n",
    "\n",
    "Input IDs are a fundamental component of BERT's preprocessing of text data. Each input ID is a numerical representation of a token in the BERT tokenizer's vocabulary. When a sentence is processed, the BERT tokenizer first splits the text into tokens (words or subwords), then maps each token to its corresponding ID based on the tokenizer's vocabulary. This conversion is crucial because neural networks, including BERT, operate on numerical data rather than raw text. The sequence of input IDs thus represents the sequence of tokens in the original sentence. Additionally, special tokens are added: `[CLS]` at the beginning of each sequence, which is used for classification tasks, and `[SEP]` to separate different sentences or to mark the end of a single sentence.\n",
    "\n",
    "**BERT's Attention Masks:**\n",
    "\n",
    "Attention masks in BERT serve the purpose of informing the model which parts of the input are actual data and which are padding. This distinction is necessary because, in batch processing, all input sequences need to be of the same length. To achieve this, sequences shorter than the maximum length are padded with a special token (`[PAD]`). However, these padding tokens should not influence the model's understanding of the actual content. Attention masks are binary (0 or 1) sequences where 1 indicates a real token and 0 indicates a padding token. During the model's processing, the attention mechanism can use these masks to focus only on the relevant parts of the input, effectively ignoring the padding. This ensures that the variable length of input texts does not affect the model's performance and allows BERT to handle inputs of varying lengths effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uPAlk4qAPZ9z",
    "outputId": "09d11f3a-480a-49c9-d676-03def617e324"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2614: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This code cell performs the following operations:\n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "# Initialize BERT model\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "\n",
    "bert_model.cuda()  # Move model to GPU if available\n",
    "\n",
    "# Function for BERT tokenization\n",
    "def bert_tokenization(dataset, use_metadata=False):\n",
    "    sentences = dataset[\"sentence\" if use_metadata else \"statement\"].values\n",
    "    labels = dataset[\"label_to_predict\"].values\n",
    "    max_length = 256\n",
    "\n",
    "    # Tokenization and data preparation\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sent in sentences:\n",
    "        encoded_dict = bert_tokenizer.encode_plus(\n",
    "            str(sent),\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "        )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert lists into tensors\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return input_ids, attention_masks, labels\n",
    "\n",
    "# Tokenize datasets\n",
    "train_input_ids, train_attention_masks, train_labels = bert_tokenization(cleaned_train_data)\n",
    "test_input_ids, test_attention_masks, test_labels = bert_tokenization(cleaned_test_data)\n",
    "val_input_ids, val_attention_masks, val_labels = bert_tokenization(cleaned_val_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setting up dataloaders for the BERT model\n",
    "\n",
    "This code cell is focused on setting up data loaders for the BERT model and configuring the training process. First, it defines a function create_dataloader to create a DataLoader for a given dataset, which automates the process of batching, shuffling (for training data), and organizing the data for efficient processing. TensorDatasets for training, validation, and testing are created by combining the respective input IDs, attention masks, and labels into a single dataset. These datasets are then passed into the create_dataloader function with a specified batch size to create DataLoaders for each dataset. This setup allows for efficient iteration over the dataset during training and evaluation. Furthermore, the code configures an optimizer (AdamW) with specific learning rate and epsilon values for the training process. It also sets up a learning rate scheduler (get_linear_schedule_with_warmup) to adjust the learning rate during training, which is a common practice to improve training efficiency and model performance. The total number of training steps is calculated based on the number of epochs and the length of the training DataLoader, ensuring that the learning rate scheduler is appropriately scaled for the duration of the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IUD89PVhP7w8",
    "outputId": "26682587-1b87-451b-d614-48f10d15afc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# This code cell performs the following operations:\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Function to create DataLoader\n",
    "def create_dataloader(dataset, batch_size, is_train=True):\n",
    "    sampler = RandomSampler(dataset) if is_train else SequentialSampler(dataset)\n",
    "    return DataLoader(dataset, sampler=sampler, batch_size=batch_size)\n",
    "\n",
    "# Combine the training inputs into a TensorDataset.\n",
    "bert_train_dataset = TensorDataset( train_input_ids, train_attention_masks, train_labels)\n",
    "\n",
    "# Combine the validation inputs into a TensorDataset.\n",
    "bert_val_dataset = TensorDataset(val_input_ids,val_attention_masks,val_labels)\n",
    "\n",
    "# Combine the test inputs into a TensorDataset.\n",
    "bert_test_dataset = TensorDataset(test_input_ids,test_attention_masks,test_labels)\n",
    "# DataLoaders for train, validation, and test datasets\n",
    "batch_size = 16\n",
    "train_dataloader = create_dataloader(bert_train_dataset, batch_size, is_train=True)\n",
    "validation_dataloader = create_dataloader(bert_val_dataset, batch_size, is_train=False)\n",
    "test_dataloader = create_dataloader(bert_test_dataset, batch_size, is_train=False)\n",
    "\n",
    "# Optimizer and Scheduler setup\n",
    "learning_rate = 2e-5\n",
    "epsilon = 1e-2\n",
    "optimizer = AdamW(bert_model.parameters(), lr=learning_rate, eps=epsilon)\n",
    "\n",
    "epochs = 5\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training and evaluating the model\n",
    "\n",
    "This code cell implements the training and evaluation process for a BERT model on a given dataset. The training is conducted over a specified number of epochs, where each epoch represents a complete pass over the entire training dataset. The train_epoch function is defined to handle the training for one epoch. Within this function, the model is set to training mode, and then iterates over batches of data from the training DataLoader. For each batch, it loads the input IDs, attention masks, and labels onto the designated device (GPU if available, otherwise CPU). The model's gradients are reset to zero at the beginning of processing each batch to prevent accumulation from previous iterations. The model then performs a forward pass with the input data and computes the loss. This loss is used to perform a backward pass, calculating the gradients for each model parameter. Finally, the optimizer updates the model's weights based on these gradients. The average loss across all batches is computed and returned for monitoring the training progress.\n",
    "\n",
    "The evaluate_model function is used to assess the model's performance on the validation set. The model is set to evaluation mode, which disables gradient calculations and certain layers like dropout, making the model more consistent and less prone to randomness in its predictions. The function iterates over the validation DataLoader, and for each batch, it performs a forward pass without gradient calculation to obtain the model's predictions (logits). These logits, which are the raw output scores for each class, are detached from the computation graph and converted to a NumPy array for further processing. The true labels are also gathered in a similar manner. The function then returns the aggregated predictions and true labels for all batches. After each training epoch, the model is evaluated on the validation set, and the results, including a classification report and confusion matrix, are printed out. This provides insights into the model's performance, such as its accuracy, precision, recall, and F1 score for each class, as well as how well it distinguishes between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LKFFp9z1P9xo",
    "outputId": "f935ec7b-1e73-4d7c-884d-c4a1b24e3f27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Training loss: 0.6729\n",
      "Validation Results\n",
      "{'0': {'precision': 0.6565774155995343, 'recall': 1.0, 'f1-score': 0.7926914968376669, 'support': 564}, '1': {'precision': 1.0, 'recall': 0.006734006734006734, 'f1-score': 0.013377926421404682, 'support': 297}, 'accuracy': 0.6573751451800233, 'macro avg': {'precision': 0.8282887077997672, 'recall': 0.5033670033670034, 'f1-score': 0.4030347116295358, 'support': 861}, 'weighted avg': {'precision': 0.775040258302134, 'recall': 0.6573751451800233, 'f1-score': 0.5238690457184684, 'support': 861}}\n",
      "[[564   0]\n",
      " [295   2]]\n",
      "Epoch 2/5 - Training loss: 0.6569\n",
      "Validation Results\n",
      "{'0': {'precision': 0.6558139534883721, 'recall': 1.0, 'f1-score': 0.7921348314606742, 'support': 564}, '1': {'precision': 1.0, 'recall': 0.003367003367003367, 'f1-score': 0.006711409395973153, 'support': 297}, 'accuracy': 0.6562137049941928, 'macro avg': {'precision': 0.827906976744186, 'recall': 0.5016835016835017, 'f1-score': 0.3994231204283237, 'support': 861}, 'weighted avg': {'precision': 0.7745401507171219, 'recall': 0.6562137049941928, 'f1-score': 0.5212048008529898, 'support': 861}}\n",
      "[[564   0]\n",
      " [296   1]]\n",
      "Epoch 3/5 - Training loss: 0.6485\n",
      "Validation Results\n",
      "{'0': {'precision': 0.6719319562575942, 'recall': 0.9804964539007093, 'f1-score': 0.797404470079308, 'support': 564}, '1': {'precision': 0.7105263157894737, 'recall': 0.09090909090909091, 'f1-score': 0.16119402985074627, 'support': 297}, 'accuracy': 0.6736353077816493, 'macro avg': {'precision': 0.6912291360235339, 'recall': 0.5357027724049, 'f1-score': 0.47929924996502715, 'support': 861}, 'weighted avg': {'precision': 0.6852449931692877, 'recall': 0.6736353077816493, 'f1-score': 0.5779451196171909, 'support': 861}}\n",
      "[[553  11]\n",
      " [270  27]]\n",
      "Epoch 4/5 - Training loss: 0.6418\n",
      "Validation Results\n",
      "{'0': {'precision': 0.7025495750708215, 'recall': 0.8794326241134752, 'f1-score': 0.7811023622047245, 'support': 564}, '1': {'precision': 0.5612903225806452, 'recall': 0.29292929292929293, 'f1-score': 0.3849557522123894, 'support': 297}, 'accuracy': 0.6771196283391405, 'macro avg': {'precision': 0.6319199488257333, 'recall': 0.5861809585213841, 'f1-score': 0.583029057208557, 'support': 861}, 'weighted avg': {'precision': 0.6538225158494714, 'recall': 0.6771196283391405, 'f1-score': 0.6444524862840235, 'support': 861}}\n",
      "[[496  68]\n",
      " [210  87]]\n",
      "Epoch 5/5 - Training loss: 0.6321\n",
      "Validation Results\n",
      "{'0': {'precision': 0.6986301369863014, 'recall': 0.9042553191489362, 'f1-score': 0.7882534775888717, 'support': 564}, '1': {'precision': 0.5877862595419847, 'recall': 0.25925925925925924, 'f1-score': 0.35981308411214946, 'support': 297}, 'accuracy': 0.6817653890824622, 'macro avg': {'precision': 0.6432081982641431, 'recall': 0.5817572892040977, 'f1-score': 0.5740332808505105, 'support': 861}, 'weighted avg': {'precision': 0.6603947925020248, 'recall': 0.6817653890824622, 'f1-score': 0.6404639341944622, 'support': 861}}\n",
      "[[510  54]\n",
      " [220  77]]\n"
     ]
    }
   ],
   "source": [
    "# This code cell performs the following operations:\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Function for training a single epoch\n",
    "def train_epoch(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch_input_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        # Clears old gradients from the last step (if they exist). This is crucial because by default, gradients are accumulated in PyTorch.\n",
    "        model.zero_grad()\n",
    "        outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_attention_mask, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        '''\n",
    "        loss.backward()\n",
    "        This computes the gradient of the loss with respect to the model parameters. These gradients are used to update the model weights.\n",
    "\n",
    "        optimizer.step()\n",
    "        This updates the model's parameters based on the gradients calculated during the backward pass.\n",
    "        '''\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# Function for evaluating the model\n",
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    # Tells pytorch not to compute and store gradients\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch_input_ids = batch[0].to(device)\n",
    "            batch_attention_mask = batch[1].to(device)\n",
    "            batch_labels = batch[2].to(device)\n",
    "\n",
    "            outputs = model(batch_input_ids, token_type_ids=None, attention_mask=batch_attention_mask)\n",
    "\n",
    "            # The logits are the raw, unnormalized scores output by the model for each class.\n",
    "            logits = outputs.logits\n",
    "\n",
    "            # The logits are detached from the current computation graph (to prevent gradient calculations) and moved to the CPU as a NumPy array.\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "\n",
    "            # Similarly, the true labels are moved to the CPU as a NumPy array.\n",
    "            label_ids = batch_labels.to('cpu').numpy()\n",
    "\n",
    "            predictions.append(logits)\n",
    "            true_labels.append(label_ids)\n",
    "\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_labels = np.concatenate(true_labels, axis=0)\n",
    "    return predictions, true_labels\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bert_model.to(device)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    avg_train_loss = train_epoch(bert_model, train_dataloader, optimizer, device)\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} - Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on the validation set\n",
    "    predictions, true_labels = evaluate_model(bert_model, validation_dataloader, device)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    print(\"Validation Results\")\n",
    "    cls_report = classification_report(true_labels, predictions, output_dict=True)\n",
    "    conf_matrix = confusion_matrix(true_labels, predictions)\n",
    "    print(cls_report)\n",
    "    print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell generates a heatmap visualization of the confusion matrix obtained from evaluating a BERT model. The confusion matrix is a useful tool in classification tasks to understand the performance of a model, showing how the predicted class labels compare to the true class labels. The heatmap, created using Seaborn's heatmap function, displays the number of instances for each combination of predicted and actual class labels, providing a clear and intuitive visual representation of where the model is performing well and where it is making errors. The labels 'Predicted' and 'Truth' on the x-axis and y-axis help identify the model's predictions versus the actual labels. This kind of visualization is helpful for quickly assessing the accuracy and error patterns of a classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 467
    },
    "id": "LQcManQozAO7",
    "outputId": "40505a83-c495-475d-9a9e-c29191237842"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(50.722222222222214, 0.5, 'Truth')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This code cell performs the following operations:\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "sn.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Truth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code cell prints the classification report of the model in a tabular format. A classification report in data science is a summary of the performance metrics for a classification model. It typically includes precision (the ratio of correctly predicted positive observations to the total predicted positives), recall or sensitivity (the ratio of correctly predicted positive observations to all actual positives), F1 score (a weighted average of precision and recall), and support (the number of actual occurrences of each class). Additionally, it often presents the overall accuracy of the model. These metrics provide a comprehensive view of a model's performance, highlighting its strengths and weaknesses in predicting different classes, and are essential for evaluating and fine-tuning classification algorithms, especially in cases where the dataset might be imbalanced or where certain types of prediction errors are more consequential than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JAw77TphzTOW",
    "outputId": "47b0f5eb-98d7-491c-e682-1bcafe59248e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score     support\n",
      "0              0.698630  0.904255  0.788253  564.000000\n",
      "1              0.587786  0.259259  0.359813  297.000000\n",
      "accuracy       0.681765  0.681765  0.681765    0.681765\n",
      "macro avg      0.643208  0.581757  0.574033  861.000000\n",
      "weighted avg   0.660395  0.681765  0.640464  861.000000\n"
     ]
    }
   ],
   "source": [
    "# This code cell performs the following operations:\n",
    "import ast\n",
    "report_df = pd.DataFrame(cls_report).transpose()\n",
    "\n",
    "print(report_df)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
